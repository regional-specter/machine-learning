{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d295094b",
   "metadata": {},
   "source": [
    "## **Gradient Boosting and Ensemble Methods**\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that an combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor\n",
    "\n",
    "Gradient boosting works by sequentially reducing the residuals of a model. At every step, you build a new model that predicts:\n",
    "- What the previous models got wrong\n",
    "- Using the gradient of the loss function (hence “gradient” boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644ab12",
   "metadata": {},
   "source": [
    "## **AdaBoost**\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more on the hard cases\n",
    "\n",
    "When training an AdaBoost classifier, the algorithm first trains a base classifier and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains the second classifier using the updated weights and again makes predictions on the training set and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba408b",
   "metadata": {},
   "source": [
    "## **Gradient Boosting**\n",
    "\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost, this method tried to fit the new predictor to the residual errors made by the previous predictor"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
